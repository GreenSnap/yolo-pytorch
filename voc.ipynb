{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "from src.utils.datasets.voc import VOCDetection\n",
    "from src.utils.datasets.voc  import VOCMerge\n",
    "\n",
    "from src.utils.datasets.ggimages import OpenImage\n",
    "from src.utils.datasets.transform import RandomHorizontalFlip, Resize, Compose, XyToCenter\n",
    "import torchvision.transforms as transforms\n",
    "from src.utils.display.images import imshow, result_show\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils.datasets.adapter import convert_data\n",
    "import numpy as np\n",
    "from src.network.yolo import Yolo\n",
    "from src.config import VOC_ANCHORS\n",
    "from src.utils.process_boxes import preprocess_true_boxes\n",
    "from src.config import IOU_THRESHOLD, TENSORBOARD_PATH\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "general_transform = Compose([\n",
    "    Resize((448, 448)),\n",
    "    XyToCenter()\n",
    "])\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [\n",
    "                                     0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "general_transform = Compose([\n",
    "])\n",
    "\n",
    "\n",
    "root = '/data/data/'\n",
    "ds = VOCMerge(root, 'train', dataset_name='VOC_FULL', oversample=True, oversample_len=100, general_transform=general_transform, read_img=False)\n",
    "print(len(ds))\n",
    "\n",
    "val_ds = VOCMerge(root, 'val', dataset_name='VOC_FULL', general_transform=general_transform, read_img=False)\n",
    "print(len(val_ds))\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_data_loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "val_data_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=5, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in ds.sub_class_dict.items():\n",
    "    print(k)\n",
    "    v.clean(name=k, commit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.label_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
